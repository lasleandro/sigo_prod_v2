{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials ok!\n",
      "Credentials not ok! Asking for one...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "\n",
    "## redirect output:\n",
    "#sys.stdout = open('output_DataCollector.dat', 'w')\n",
    "\n",
    "\n",
    "## modules\n",
    "sys.path.insert(0, './_modules')\n",
    "from QRCode_module import decode\n",
    "from scanner_module import scanner\n",
    "from regex_module import FindEmail, FindNumbersBraket, FindLawsuit\n",
    "from general_functions import DateConv\n",
    "from Gmail_module import GetAttachments_v2, GetMessage_v2, ListMessagesMatchingQuery, GetBody,\\\n",
    "                         FDS, PrintMail, GetCredentials, MailData, GetBody_v2\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import schedule\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "## log\n",
    "#logname = 'logging_output_DataCollector.dat'\n",
    "#logging.basicConfig(filename = logname, filemode = 'a', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "def mailId_query(control_path, new_query = True, query_type = 'day'):\n",
    "    '''\n",
    "    Query for new emails starting from the current day.\n",
    "    Input: i) the path to the directory containing mid_base.csv;\n",
    "           ii) new_query = True or False (If False, only looks at the mail list stored locally)\n",
    "           iii) query_type = 'day' or 'full' (date range starting from 2018)\n",
    "    Output: i) a list with the history all emails Id, with the ones of the current day appended.\n",
    "    Files: i) creates/updates mid_base.csv\n",
    "    '''\n",
    "\n",
    "    ## path to the csv where all mail ids are stored\n",
    "    file_path = os.path.join(control_path, 'mid_base.csv')\n",
    "\n",
    "\n",
    "    if new_query == True:\n",
    "\n",
    "        ## query:\n",
    "        ## yyyy-mm-dd format or yyyy/mm/dd\n",
    "        if query_type == 'day':\n",
    "            start_time2 = (datetime.today()).strftime('%Y-%m-%d')\n",
    "        elif query_type == 'full':\n",
    "            start_time2 = '2018-01-01'\n",
    "        else:\n",
    "            print('You need specify a \"day\" or \"full\" query')\n",
    "            print('Following with a full query.')\n",
    "            start_time2 = '2018-01-01'\n",
    "\n",
    "        end_time2 = (datetime.today() + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "        mail_list = None\n",
    "        while mail_list is None:\n",
    "            try:\n",
    "                mail_list = ListMessagesMatchingQuery(query='after:{} before:{}'.format(start_time2, end_time2))\n",
    "                print('Mail counting from {} to {}: {}'.format(start_time2, end_time2, len(mail_list)))\n",
    "            except Exception as e:\n",
    "                logging.error(\"Exception occurred\", exc_info=True)\n",
    "                sleeptime = 600\n",
    "                logging.warning(\"Sleeping for {} seconds.\".format(sleeptime))\n",
    "                time.sleep(sleeptime)\n",
    "                pass\n",
    "\n",
    "\n",
    "        ## saving to a dataframe\n",
    "        df_mid = pd.DataFrame(mail_list)\n",
    "        df_mid['after'] = start_time2\n",
    "        df_mid['before'] = end_time2\n",
    "\n",
    "\n",
    "        ## appending or creating the csv\n",
    "        if os.path.exists(file_path) == True:\n",
    "            with open(file_path, 'a') as f:\n",
    "                df_mid.to_csv(f, header=False, sep ='\\t', encoding='utf-8', index=False)\n",
    "        else:\n",
    "            df_mid.to_csv(file_path, sep ='\\t', encoding='utf-8',index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## reading the csv from directory\n",
    "    df_mid = pd.read_csv(file_path, sep = '\\t', encoding = 'utf-8')\n",
    "\n",
    "    ## dropping eventual duplicates\n",
    "    df_mid = df_mid.drop_duplicates(subset = ['id']).reset_index(drop = True)\n",
    "\n",
    "    ## rewriting the dataframe to the directory\n",
    "    df_mid.to_csv(file_path, sep = '\\t', encoding = 'utf-8', index = False)\n",
    "\n",
    "    ## extracting the mail list\n",
    "    all_mid_list = list(set(df_mid['id'].tolist()))\n",
    "    print('All mails counting: {}'.format(len(all_mid_list)))\n",
    "\n",
    "\n",
    "\n",
    "    return all_mid_list\n",
    "###################################################################\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "def TheGreatFilter(all_mid_list, control_path):\n",
    "    '''\n",
    "    Given a list of mail ids, finds the ones that didn't get collected yet.\n",
    "    Input: a list of all email ids\n",
    "    Output: a list with emails to be collected\n",
    "    Files: reads mail_data.csv\n",
    "    '''\n",
    "\n",
    "    ## checking if there is mail to be collected\n",
    "    collected_file = os.path.join(control_path, 'mail_data.csv')\n",
    "    if os.path.exists(collected_file):\n",
    "        df_collected = pd.read_csv(os.path.join(control_path, 'mail_data.csv'), sep = '\\t', encoding = 'utf-8')\n",
    "        collected_list = list(set(df_collected['mail_id'].tolist()))\n",
    "\n",
    "    else:\n",
    "        collected_list = []\n",
    "\n",
    "    ## excluding the mails already collected\n",
    "    mail_list = list(set(all_mid_list) - set(collected_list))\n",
    "    mail_counter = len(mail_list)\n",
    "    print(mail_counter)\n",
    "\n",
    "\n",
    "    ## for long lists, it is more safe to split them into small pieces to collect and store them\n",
    "    nparts = 100\n",
    "    if mail_counter >= nparts:\n",
    "        splitted_mail_list = np.array_split(mail_list, nparts)\n",
    "    else:\n",
    "        nparts = 1\n",
    "        splitted_mail_list = [mail_list.copy()]\n",
    "\n",
    "\n",
    "\n",
    "    return splitted_mail_list, mail_counter\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "def TheCollector(splitted_mail_list, control_path, attachments_path):\n",
    "    '''\n",
    "    Collects emails and attachments from a given list of mail ids.\n",
    "    Input: a list of lists of email ids (a list of ids splitted into chuncks)\n",
    "    Output: i) df = dataframe containing data from emails\n",
    "            ii) df_attachments = df containing the files attached to a given mail\n",
    "            iii) df_to = df containing all destinataries of a given mail\n",
    "    Files: i) creates/updates mail_data.csv\n",
    "           ii) creates/updates att_data.csv\n",
    "           iii) creates/updates to_data.csv\n",
    "\n",
    "    '''\n",
    "\n",
    "    nparts = len(splitted_mail_list)\n",
    "    counter = 0\n",
    "    time_list = []\n",
    "    global_start_time = time.time()\n",
    "    for subset_mail_list in splitted_mail_list:\n",
    "        start_time = time.time()\n",
    "        print('>>> Subset {}'.format(counter+1))\n",
    "\n",
    "        try:\n",
    "            df, df_attachments, df_to = FDS(subset_mail_list, attachments_path)\n",
    "        except Exception as e:\n",
    "            print('Error')\n",
    "            logging.error(\"Exception occurred\", exc_info=True)\n",
    "\n",
    "\n",
    "        ## saving data\n",
    "        file_path = os.path.join(control_path, 'mail_data.csv')\n",
    "        att_path = os.path.join(control_path, 'att_data.csv')\n",
    "        destinatary_path = os.path.join(control_path, 'to_data.csv')\n",
    "        if os.path.exists(file_path) == True:\n",
    "            with open(file_path, 'a') as f:\n",
    "                df.to_csv(f, header = False, sep ='\\t', encoding = 'utf-8', index=False)\n",
    "\n",
    "            with open(att_path, 'a') as f:\n",
    "                df_attachments.to_csv(f, header = False, sep ='\\t', encoding = 'utf-8', index=False)\n",
    "\n",
    "            with open(destinatary_path, 'a') as f:\n",
    "                df_to.to_csv(f, header = False, sep ='\\t', encoding = 'utf-8', index=False)\n",
    "\n",
    "        else:\n",
    "            df.to_csv(file_path, sep ='\\t', encoding = 'utf-8',index=False)\n",
    "            df_attachments.to_csv(att_path, sep ='\\t', encoding = 'utf-8', index=False)\n",
    "            df_to.to_csv(destinatary_path, sep ='\\t', encoding = 'utf-8', index=False)\n",
    "\n",
    "\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "        counter += 1\n",
    "        time.sleep(5)\n",
    "\n",
    "        dt = time.time() - start_time\n",
    "        time_list.append(dt)\n",
    "        forecasting = np.mean(time_list)*nparts\n",
    "\n",
    "        print('Total time elapsed: {}'.format(time.time() - global_start_time))\n",
    "        print('Step time: {}'.format(dt))\n",
    "        print('Forecasting: {}'.format(forecasting))\n",
    "        print(' ')\n",
    "        print('>>>>>>>>>>>>>>')\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "\n",
    "    return df, df_attachments, df_to\n",
    "######################################################\n",
    "\n",
    "\n",
    "######################################################\n",
    "def Orchestrator(control_path, attachments_path, query_type):\n",
    "    '''\n",
    "    Orchestrates all pipeline of mail extraction.\n",
    "    Input: the control path, attachments path and the type of query (full or day)\n",
    "    Output: a log csv\n",
    "    Files: i) inserts new mail data into the MySQL screening_table\n",
    "           ii) generates a log csv containing the following infos: 'date',\n",
    "           'step_time', 'new_mail_counter', 'total_mail_counter' and 'query_type'.\n",
    "    '''\n",
    "\n",
    "    global_start_time = time.time()\n",
    "    ## looking for new emails:\n",
    "    print('Looking for new emails')\n",
    "    logging.info('Looking for new emails')\n",
    "    start_time = time.time()\n",
    "    all_mid_list = mailId_query(control_path, query_type = query_type)\n",
    "    dt = time.time() - start_time\n",
    "    print('Time spent: {}'.format(dt))\n",
    "    print(' ')\n",
    "\n",
    "    ## filtering new email:\n",
    "    print('Filtering new emails')\n",
    "    logging.info('Filtering new emails')\n",
    "    start_time = time.time()\n",
    "    splitted_mail_list, mail_counter = TheGreatFilter(all_mid_list, control_path)\n",
    "    dt = time.time() - start_time\n",
    "    print('Time spent: {}'.format(dt))\n",
    "    print(' ')\n",
    "\n",
    "    ## Getting mail data and attachments\n",
    "    start_time = time.time()\n",
    "    print('Getting mail data and attachments')\n",
    "    logging.info('Getting mail data and attachments')\n",
    "    if mail_counter > 0:\n",
    "        df, df_attachments, df_to = TheCollector(splitted_mail_list, control_path, attachments_path)\n",
    "\n",
    "        df = df.drop('mail_date_received_full', axis = 1)\n",
    "\n",
    "        ## converting time to MySQL format\n",
    "        df['mail_date_received'] = pd.to_datetime(df['mail_date_received'])\n",
    "        df['mail_hms_received'] = df['mail_hms_received'].astype(str)\n",
    "        df['mail_hms_received'] = pd.to_datetime(df['mail_hms_received']).dt.time\n",
    "        df['mail_body'] = df['mail_body'].str.encode('utf-8')\n",
    "        df['has_attachment'] = df['has_attachment'].astype(str)\n",
    "\n",
    "\n",
    "        \n",
    "        ## getting the mail ids already present in the DB:\n",
    "        df_db = get_table('screening_table', 'mail_id')\n",
    "        db_mail_id_list = list(set(df_db['mail_id'].tolist()))        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## dropping mails already in DB in order to avoid repeated entries:\n",
    "        df = df[df['mail_id'].isin(db_mail_id_list) == False].reset_index(drop = True)\n",
    "        df_attachments = df_attachments[df_attachments['mail_id'].isin(db_mail_id_list) == False].reset_index(drop = True)\n",
    "        df_to = df_to[df_to['mail_id'].isin(db_mail_id_list) == False].reset_index(drop = True)\n",
    "        \n",
    "\n",
    "        # create sqlalchemy engine\n",
    "        engine = create_engine(\"mysql://{user}:{pw}@localhost/{db}?charset=utf8mb4\"\n",
    "                               .format(user=\"root\",\n",
    "                                       pw=\"Le@ndro159753\",\n",
    "                                       db=\"screening\"), encoding=\"utf8\")\n",
    "        logging.info('Loading MySQL database.')\n",
    "\n",
    "        # Insert whole DataFrame into MySQL\n",
    "        df.to_sql('screening_table', con = engine, if_exists = 'append', index = False, chunksize = 1000)\n",
    "        logging.info('Data written to MySQL database.')\n",
    "        engine.dispose()\n",
    "\n",
    "\n",
    "    else:\n",
    "        ## defining empty dfs for return purposes:\n",
    "        df = pd.DataFrame()\n",
    "        df_attachments = pd.DataFrame()\n",
    "        df_to = pd.DataFrame()\n",
    "        \n",
    "        print('No new mail at this time!')\n",
    "        logging.info('No new mail found.')\n",
    "\n",
    "\n",
    "\n",
    "    dt = time.time() - start_time\n",
    "    print('Time spent: {}'.format(dt))\n",
    "    print(' ')\n",
    "    dt_final = time.time() - global_start_time\n",
    "    print('Total time spent: {}'.format(dt_final))\n",
    "\n",
    "\n",
    "    ## for log purposes:\n",
    "    df_log = pd.DataFrame(columns = ['date', 'step_time', 'new_mail_counter',\n",
    "                                     'total_mail_counter', 'query_type'])\n",
    "    df_log.at[0, 'date'] = str(datetime.today())\n",
    "    df_log.at[0, 'step_time'] = dt_final\n",
    "    df_log.at[0, 'new_mail_counter'] = mail_counter\n",
    "    df_log.at[0, 'total_mail_counter'] = len(all_mid_list)\n",
    "    df_log.at[0, 'query_type'] = query_type\n",
    "\n",
    "    log_path = os.path.join(control_path, 'log.csv')\n",
    "    if os.path.exists(log_path) == True:\n",
    "        with open(log_path, 'a') as f:\n",
    "            df_log.to_csv(f, header = False, sep ='\\t', encoding = 'utf-8', index=False)\n",
    "    else:\n",
    "        df_log.to_csv(log_path, sep ='\\t', encoding = 'utf-8', index=False)\n",
    "    logging.info('Log saved to .csv')\n",
    "\n",
    "\n",
    "\n",
    "    return df, df_attachments, df_to\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "def get_table(tablename, option = 'full'):\n",
    "    \n",
    "    '''\n",
    "    Get the table and converts to a pandas dataframe.\n",
    "    Input: i) tablename\n",
    "           ii) option = 'full' (full table) or option = column_name \n",
    "    Output: i) df\n",
    "\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    table_rows = None\n",
    "    \n",
    "    connection = mysql.connector.connect(host='localhost',\n",
    "                                         database='screening',\n",
    "                                         user='root',\n",
    "                                         password='Le@ndro159753')    \n",
    "    \n",
    "    \n",
    "    db_cursor = connection.cursor()\n",
    "    db_cursor.execute(\"SHOW TABLES\")\n",
    "    tables_list = db_cursor.fetchall()\n",
    "    tables_list = [item[0] for item in tables_list] \n",
    "    \n",
    "    ## check if the inputed table name is in the database\n",
    "    if tablename in tables_list:\n",
    "    \n",
    "        db_cursor = connection.cursor()\n",
    "        ## getting all columns names:\n",
    "        db_cursor.execute(\"SHOW columns FROM {}\".format(tablename))\n",
    "        columns_list = [column[0] for column in db_cursor.fetchall()]\n",
    "        if option == 'full':\n",
    "            db_cursor.execute(\"SELECT * FROM {}\".format(tablename))\n",
    "            table_rows = db_cursor.fetchall()\n",
    "        else:\n",
    "            if option in columns_list:\n",
    "                columns_list = [option]\n",
    "                db_cursor.execute(\"SELECT {} FROM {}\".format(option, tablename))\n",
    "                table_rows = db_cursor.fetchall()\n",
    "            else:\n",
    "                print('Column name not present in the table.')\n",
    "                logging.error('Column name not present in the table.')\n",
    "                \n",
    "    else:\n",
    "        print('Table not present in the database.')\n",
    "        logging.error('Table not present in the database.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## writing rows to a dataframe\n",
    "    if table_rows is not None:\n",
    "        df_test = pd.DataFrame(table_rows)   \n",
    "        df_test.columns = columns_list\n",
    "    else:\n",
    "        df_test = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    ## closing the connection\n",
    "    connection.close()\n",
    "   \n",
    "    return df_test\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leandro/Desktop/Dropbox/sigo\n",
      "/home/leandro/Desktop/Dropbox/sigo/_attachments\n",
      "Folder already exists!\n",
      "/home/leandro/Desktop/Dropbox/sigo/_collect_control\n",
      "Folder already exists!\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "\n",
    "## specifying the path to attachments:\n",
    "attachments_path = os.path.join(cwd, '_attachments')\n",
    "print(attachments_path)\n",
    "if os.path.isdir(attachments_path) == False:\n",
    "    os.mkdir(attachments_path)\n",
    "    print('Folder created!')\n",
    "else:\n",
    "    print('Folder already exists!')\n",
    "logging.info('Creating/verifying attachments folder')\n",
    "\n",
    "\n",
    "## specifying the path to collect control:\n",
    "control_path = os.path.join(cwd, '_collect_control')\n",
    "print(control_path)\n",
    "if os.path.isdir(control_path) == False:\n",
    "    os.mkdir(control_path)\n",
    "    print('Folder created!')\n",
    "else:\n",
    "    print('Folder already exists!')\n",
    "logging.info('Creating/verifying control folder')\n",
    "\n",
    "\n",
    "\n",
    "## getting the process id\n",
    "pid = os.getpid()\n",
    "with open(os.path.join(control_path, 'pid_DataCollector.dat'), 'w') as f:\n",
    "    f.write(str(pid)+'\\n')\n",
    "f.close()\n",
    "logging.info('Saving the PID.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scheduler\n",
    "##############################\n",
    "def job():\n",
    "    '''\n",
    "    The job for the scheduler\n",
    "    '''\n",
    "\n",
    "    ## choose randomly between a \"full\" or daily lookout\n",
    "    query_type = random.choice(['full', 'day', 'day'])\n",
    "    df, df_attachments, df_to = Orchestrator(control_path, attachments_path, query_type  = query_type)\n",
    "\n",
    "    return\n",
    "##############################\n",
    "\n",
    "time_interval = 5\n",
    "schedule.every(time_interval).minutes.do(job)\n",
    "\n",
    "while True:\n",
    "\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
